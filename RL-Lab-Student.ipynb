{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement-Learning Reference\n",
    "Reference Implementation for Reinforcement Assignment**\n",
    "\n",
    "### Source:\n",
    "These are the files required to build your reinforcement learning algorithm. \n",
    "\n",
    "- [common.py](common.py) with constants\n",
    "- [util.py](util.py) with util functions\n",
    "- [game.py](game.py) with drawing calls\n",
    "- [environment.py](environment.py) contains the scenario behavior\n",
    "- [agent.py](agent.py) contains training components, such as environment interaction and previous state\n",
    "\n",
    "### Assignment\n",
    "The goal of this assignment is to implement the core of the Q-Learning algorithm. You will be responsible for implementing three distinct methods:\n",
    "- The exploration function(**f()** method)\n",
    "- The Q-Learning update method (**get_action()** method)\n",
    "- Implemnet a decreasing function for the learning rate (**alpha()** method)\n",
    "\n",
    "In this scenario we help the agent, via reinforcement learning, to navigate and maximise rewards within a map, aiming to reach the move between an initial state and the goal state, represented by a treasure chest. In some scenarios there will be a rupee that the agent can gather. The rewards are +50 for reaching the chest, +40 for getting the rupee and -1 for any other tile.\n",
    "\n",
    "This assignment is not graded. Thus no tests are provided.\n",
    "\n",
    "### Execution\n",
    "The execution of this assignment can be done entirely in this Jupyter Notebook, or in two distinct python files. If you want to program/test outside of Jupyter just follow these instructions. Please note that currently there is a problem with pygame (a package responsible for displaying the agent moving in the environment) and Jupyter, which causes the jupyter kernel to crash after closing the pygame window.\n",
    "\n",
    "In order to test your code and get the convergence episode, you can use the environment.py file:\n",
    "```\n",
    "python environment.py [Map]\n",
    "```\n",
    "\n",
    "To check the converged solution of your algorithm, you can run the GUI to see the agent executing the learned policy in each map.\n",
    "```\n",
    "python game.py [Map]\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "In this file, we provide the basic architecture to build your Q-Learning algorithm. Additionally, you can implement your algorithm in the [link_ref.py](link_ref.py) file, if you wish to work outside of Jupyter Notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration function\n",
    "Implement the optimistic estimate function described by the following equation.\n",
    "$$\n",
    "f(u,n) = \\begin{cases}\n",
    "\t\t\t\t\tR^{+} & \\mathit{if} \\text{ }n < N_{e} \\\\\n",
    "\t\t\t\t\tu & \\mathit{otherwise}\n",
    "\t\t\t\t   \\end{cases}\n",
    "$$\n",
    "Consider that instead of just using the utility *u* in the python method as parameter, we are using **qv**, which is a Q-Value (state-action pair).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receives a q-value and returns a utility\n",
    "def f(self, qv):\n",
    "    return self.r_plus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "Implement the Q-Learning algorithm. Consider the learning rate as a fixed value for now.\n",
    "<img src=\"https://user-images.githubusercontent.com/4201145/45648565-4d0c0580-ba9f-11e8-82fd-1a4f127c1959.png\" width=\"70%\" height=\"70%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(self, state):\n",
    "    #Remove this.\n",
    "    import random\n",
    "    return random.choice(self.env.available_actions((state.x, state.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha(self,qv):\n",
    "    # Implement here a more sophisticated learning rate\n",
    "    return 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Four spaces as indentation [no tabs]\n",
    "# Standard Q-Learning implementation.\n",
    "import math, copy, random, logging\n",
    "from qvalue import *\n",
    "from common import *\n",
    "from util import *\n",
    "from agent import *\n",
    "\n",
    "class Link(Agent):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        Agent.__init__(self)\n",
    "        self.q_values = dict()\n",
    "        self.frequency = dict()\n",
    "        self.state = None\n",
    "        self.reward = None\n",
    "        self.action = NO_OP\n",
    "        self.p_state = None \n",
    "        self.p_reward = None\n",
    "        self.p_action = None\n",
    "        self.gamma = 0.9\n",
    "        self.r_plus = 50\n",
    "        self.exploration = 1\n",
    "        self.env = None\n",
    "        self.prev_qtable = dict()\n",
    "\n",
    "    def reset(self, env):\n",
    "        \"\"\"\n",
    "        Reset the state to the initial environment state\n",
    "        \"\"\"\n",
    "        self.state = env.init\n",
    "\n",
    "    def train(self, env):\n",
    "        \"\"\"\n",
    "        Execute MAX_TRAINING_EPISODES rounds or until converge.\n",
    "        \"\"\"\n",
    "        print('It will converge at', CONVERGENCE_THRESHOLD)\n",
    "\n",
    "        self.reset(env)\n",
    "        self.env = env\n",
    "\n",
    "        executions = 0\n",
    "        last_plan = []\n",
    "        while executions < MAX_TRAINING_EPISODES:\n",
    "            self.state = self.make_state(env)\n",
    "            action = self.get_action(self.state)\n",
    "            last_plan.append(action)\n",
    "            self.env.execute(action)\n",
    "            if env.terminal((self.state.x, self.state.y)):\n",
    "                executions += 1\n",
    "                \n",
    "                self.p_state = self.p_action = self.p_reward = self.state = self.action = self.reward = None\n",
    "                self.reset(env)\n",
    "\n",
    "                if self.converged():\n",
    "                    break\n",
    "                else:\n",
    "                    last_plan = []\n",
    "                    self.prev_qtable = copy.deepcopy(self.q_values)\n",
    "\n",
    "                #print('Episode', executions, ': convergence %', self.convergence)\n",
    "\n",
    "        print('Episode' , executions, ' : converged at', self.convergence)\n",
    "        print('Last plan executed: ', [ACTIONS_NAMES[x] for x in last_plan])\n",
    "        #self.return_policy()\n",
    "\n",
    "    def alpha(self, qv):\n",
    "        \"\"\"\n",
    "        Alpha value, currently returning 0.9 because it converges pretty fast. \n",
    "        \"\"\"\n",
    "        return alpha(self,qv)\n",
    "\n",
    "    def f(self, qv):\n",
    "        \"\"\"\n",
    "        Exploration function. Use maxreward if the q_value was not explored.\n",
    "        \"\"\"\n",
    "        return f(self,qv)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return get_action(self,state)   \n",
    "\n",
    "    def max_a(self, state):\n",
    "        \"\"\"\n",
    "        Standard max action implementation java style.\n",
    "        Return the max value you can obtain in a certain state.\n",
    "        \"\"\"\n",
    "        max_value = float('-inf')\n",
    "        if self.env.terminal((state.x, state.y)):\n",
    "            max_value = self.q_values[qvalue.QValue(state, NO_OP)]\n",
    "        else:\n",
    "            for action in self.env.available_actions((state.x, state.y)):\n",
    "                qv = qvalue.QValue(state, action)\n",
    "                if qv in self.q_values:\n",
    "                    q_sa = self.q_values[qv]\n",
    "                    if q_sa > max_value:\n",
    "                        max_value = q_sa\n",
    "        if max_value == float('-inf'): \n",
    "            max_value = 0.0\n",
    "        return max_value\n",
    "\n",
    "    def argmax_a(self, state):\n",
    "        \"\"\"\n",
    "        Standard argmax action implementation java style.\n",
    "        Return the best action you can perform in a certain state.\n",
    "        \"\"\"\n",
    "        a = NO_OP\n",
    "        max_value = float('-inf')\n",
    "        if state == None:\n",
    "            return a\n",
    "        for action in self.env.available_actions((state.x, state.y)):\n",
    "            qv = qvalue.QValue(state, action)\n",
    "            fvalue = self.f(qv)\n",
    "            if fvalue > max_value:\n",
    "                max_value = fvalue\n",
    "                a = action\n",
    "        return a\n",
    "\n",
    "    def make_state(self, env):\n",
    "        \"\"\"\n",
    "        Build state using position and rupees.\n",
    "        \"\"\"\n",
    "        return State(env.state[0], env.state[1], env.rupees)\n",
    "\n",
    "    def return_qvalue(self, qvalue):\n",
    "        if qvalue in self.q_values:\n",
    "            return self.q_values[qvalue]\n",
    "        return 0\n",
    "\n",
    "    def converged(self):\n",
    "        \"\"\"\n",
    "        Return True if the change between previous util table and current util table\n",
    "        are smaller than the convergence_threshold.\n",
    "        \"\"\"\n",
    "        self.convergence = self.convergence_metric()\n",
    "        return self.convergence < CONVERGENCE_THRESHOLD\n",
    "\n",
    "    def run(self, env):\n",
    "        \"\"\"\n",
    "        Execute actions.\n",
    "        \"\"\"\n",
    "        self.action = self.argmax_a(self.make_state(env))\n",
    "        #print \"Running action: \", ACTIONS_NAMES[self.action]\n",
    "        self.state, self.reward = env.execute(self.action)\n",
    "        return self.action, self.state\n",
    "\n",
    "\n",
    "    def convergence_metric(self):\n",
    "        \"\"\"\n",
    "        Return the convergence metric.\n",
    "        \"\"\"\n",
    "        prev = sum(self.prev_qtable.values())\n",
    "        curr = sum(self.q_values.values())\n",
    "        return math.sqrt(abs(curr - prev))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Agent\n",
    "In this cell, we train the agent. You can change the map and add any code you like here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from game import *\n",
    "    pg = True\n",
    "except ImportError:\n",
    "    pg = False\n",
    "    \n",
    "from environment import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "sx, sy, map_data, map_width, map_height = read_map(\"maps/medium.txt\")\n",
    "\n",
    "agt = Link()\n",
    "\n",
    "env = Environment(sx, sy, map_data, map_width, map_height)\n",
    "\n",
    "start_time = time.time()\n",
    "agt.train(env)\n",
    "elapsed_time = time.time() - start_time\n",
    "print('It took', elapsed_time,'seconds to train.' )\n",
    "if pg:\n",
    "    #Comment this line if you do not want to use the UI\n",
    "    Game(env, agt)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
